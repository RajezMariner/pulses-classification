{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference_B0_Img_Size_224.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py7KklhxHoJC"
      },
      "source": [
        "# Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yN73A132WTE"
      },
      "source": [
        "!mkdir dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBWHg2Os2vdE"
      },
      "source": [
        "!unzip \"/content/drive/My Drive/Pulse Classification/Indian Pulses.zip\" -d \"/content/dataset\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLf9Ei712xdi",
        "outputId": "3c987e16-4b00-43a7-c7f1-900bc202d61b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!pip install -q albumentations==0.4.5\n",
        "!pip install -q efficientnet_pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 122kB 5.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 634kB 47.5MB/s \n",
            "\u001b[?25h  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tr9LInY43726"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "import albumentations as A\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCZZo6H5Hunp"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DevoGHCT4CT-"
      },
      "source": [
        "def Net(model_name = 'b0', output = 14):\n",
        "    model = EfficientNet.from_pretrained(f'efficientnet-{model_name}')\n",
        "    model._fc = nn.Linear(in_features = model._fc.in_features, out_features = output, bias = True)\n",
        "    return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U08cd6UZ4EwN"
      },
      "source": [
        "def load_model(path):\n",
        "  model = Net().cuda()\n",
        "  model.load_state_dict(torch.load(path)[\"model_state_dict\"])\n",
        "  model.eval()\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYaf9FBCHzH9"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26GqnQJU5-P2"
      },
      "source": [
        "class DatasetRetriever(Dataset):\n",
        "\n",
        "    def __init__(self, image_ids, transforms=None):\n",
        "        super().__init__()\n",
        "        self.image_ids = image_ids\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image_id = self.image_ids[idx]\n",
        "        image = cv2.imread(f'{Config.DIR}/{image_id}', cv2.IMREAD_COLOR)\n",
        "\n",
        "        if self.transforms:\n",
        "            sample = {'image': image}\n",
        "            sample = self.transforms(**sample)\n",
        "            image = sample['image']\n",
        "\n",
        "        return image\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.image_ids.shape[0]"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl_EvMnV5eyw"
      },
      "source": [
        "def validation_augmentations(img_size = 224):\n",
        "    return A.Compose([\n",
        "            A.Resize(height=img_size, width=img_size, p=1.0),\n",
        "            ToTensorV2(p=1.0),\n",
        "        ], p=1.0)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYfjfbn_H23S"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8szBaUT7oU5"
      },
      "source": [
        "class Config:\n",
        "\n",
        "  DIR = \"/content/dataset\"\n",
        "\n",
        "  num_workers=4\n",
        "  batch_size=32"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C3ZCxKRH7AV"
      },
      "source": [
        "## Engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEsr12Mp8EQv"
      },
      "source": [
        "class Engine:\n",
        "\n",
        "  def __init__(self, fold):\n",
        "    \n",
        "    self.predictions = list()\n",
        "    self.model = load_model(f'/content/224_b0_{fold}.pt')\n",
        "    print(f'Model loaded for fold {fold}')\n",
        "  \n",
        "  def fit(self, validation_loader):\n",
        "    for _, x_val in enumerate(validation_loader):\n",
        "      \n",
        "      temp = list()\n",
        "      images = x_val.to(torch.device('cuda'), dtype=torch.float32)\n",
        "      \n",
        "      pred = self.model(images)\n",
        "      y_pred = nn.functional.softmax(pred, dim=1).data.cpu().numpy()\n",
        "\n",
        "      for each_pred in y_pred:\n",
        "        temp.append(each_pred.argmax())\n",
        "\n",
        "      self.predictions.extend(temp)\n",
        "    return self.predictions\n"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blg699T4H8pv"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFJ73Gww6eqY"
      },
      "source": [
        "def perform_for_fold(fold_number=0):\n",
        "\n",
        "  valid_X = df[df[\"k-fold\"] == fold_number].Path.values\n",
        "  valid_Y = df[df[\"k-fold\"] == fold_number].Label_enc.values\n",
        "\n",
        "  valid_dataset = DatasetRetriever(valid_X, validation_augmentations(img_size = 224))\n",
        "        \n",
        "  validation_loader = torch.utils.data.DataLoader(\n",
        "            valid_dataset, \n",
        "            batch_size=Config.batch_size,\n",
        "            num_workers=Config.num_workers,\n",
        "            shuffle=False,\n",
        "            sampler=SequentialSampler(valid_dataset),\n",
        "            pin_memory=False,\n",
        "        ) \n",
        "  \n",
        "  engine = Engine(fold_number)\n",
        "  pred = engine.fit(validation_loader)\n",
        "  return pred, valid_Y"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALLGMK7iIDmH"
      },
      "source": [
        "## Inference started for all folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWn8J99hICqN"
      },
      "source": [
        "df = pd.read_csv('/content/dataset_folds.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY037d7v991P",
        "outputId": "ed9d7ddd-1f1f-4301-f54f-c4597514627c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "pred_0, true_0=perform_for_fold(0)\n",
        "pred_1, true_1=perform_for_fold(1)\n",
        "pred_2, true_2=perform_for_fold(2)\n",
        "pred_3, true_3=perform_for_fold(3)\n",
        "pred_4, true_4=perform_for_fold(4)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Model loaded for fold 0\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Model loaded for fold 1\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Model loaded for fold 2\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Model loaded for fold 3\n",
            "Loaded pretrained weights for efficientnet-b0\n",
            "Model loaded for fold 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-tHy7kRIHX-"
      },
      "source": [
        "## Evaulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_kXTK5HEmPB"
      },
      "source": [
        "pred_all = np.concatenate((pred_0, pred_1, pred_2, pred_3, pred_4), axis=0)\n",
        "true_all = np.concatenate((true_0, true_1, true_2, true_3, true_4), axis=0)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rJzgD2oDdUF"
      },
      "source": [
        "import json\n",
        "\n",
        "def key_conversion(array):\n",
        "  return [INV_KEY[each]for each in array]\n",
        "\n",
        "with open('/content/label_key.json') as f:\n",
        "  KEY = json.load(f)\n",
        "\n",
        "INV_KEY = {v: k for k, v in KEY.items()}\n",
        "\n",
        "pred_label = key_conversion(pred_all)\n",
        "true_label = key_conversion(true_all)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SeKX5JHA7Ds",
        "outputId": "e27184d4-7763-42d6-8eff-1d599f706c13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(true_label, pred_label))"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                            precision    recall  f1-score   support\n",
            "\n",
            "              Adzuki Beans       0.99      0.98      0.98       632\n",
            "                Black Gram       0.96      0.97      0.97       527\n",
            "                 Chickpeas       0.99      0.99      0.99       622\n",
            "                  Dew Bean       0.98      0.97      0.98       398\n",
            "           Green Chickpeas       0.98      0.98      0.98       551\n",
            "                Green Gram       0.98      0.97      0.98       564\n",
            "               Pinto Beans       0.99      0.99      0.99       765\n",
            "          Red Kidney Beans       0.99      0.99      0.99       577\n",
            "               Red Lentils       0.99      0.99      0.99       586\n",
            "Split & Skinned Black Gram       0.99      0.99      0.99       557\n",
            "          Split Black Gram       0.96      0.95      0.96       488\n",
            "          Split Green Gram       0.97      0.97      0.97       220\n",
            "        White Kidney Beans       0.99      0.99      0.99       575\n",
            "            Yellow Lentils       0.98      0.99      0.99       664\n",
            "\n",
            "                  accuracy                           0.98      7726\n",
            "                 macro avg       0.98      0.98      0.98      7726\n",
            "              weighted avg       0.98      0.98      0.98      7726\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}